experiment_name: "demand_forecasting_exp_24"

gcp:
  project_id: "lucid-sonar-465922-s9"
  bucket_name: "labo3_bucket"
  worker_zone: "us-east1-b" # b, c, d
  node0_zone: "us-east1-d" # Fixed | Do not touch!
  region: "us-east1"
  blob_path: "data"

repository:
  url: "https://github.com/MatiLarsson/labo3-2025v.git"
  branch: "main"

jobs:
  name: "model"
  script: "run.py" 
  instance_name: "worker"
  machine_type: "n2-highmem-64" # "n2-highmem-64" -> 64 CPU, 512GB RAM
  preemptible: true
  boot_disk_size: "2000GB"
  boot_disk_type: "pd-ssd"  # type: pd-standard, pd-ssd, pd-balanced

paths:
  data_files:
    - "data/product_id_to_predict_201912.txt"
    - "data/sell-in.txt"
    - "data/tb_productos.txt"
    - "data/tb_stocks.txt"

dataset:
  clip_negative_predictions_before_aggregation: false # True to clip negative predictions before aggregatioin in bo optimization, test, and final predictions for kaggle
  calculate_tfe_only_on_kaggle_products: true # True to calculate TFE only on products that are in the Kaggle submission file, during bo optimization and final test
  train_only_on_kaggle_top_products: false # True to limit the train and test datasets to the top products required by the Kaggle submission list
  top_kaggle_products_to_train_on: 100 # Top products to consider (the top products from the submission requred product_ids with the most sales in 2019)
  min_periodo: 201701 # always applies
  future_periods_extension: 3
  positive_quantity_tn_cherry_months: 9 # applied as flag
  max_lag_periods: 13 # 35 max
  anchored_ratios_periods: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
  rolling_stats_window_sizes: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] # [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
  rolling_statistics: ['mean', 'median', 'std', 'min', 'max', 'sum', 'skewness', 'kurtosis']
  reg_slopes_window_sizes: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] # [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
  max_z_lag_periods: 13 # 35 max
  z_anchored_delta_lag_periods: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
  z_adjacent_delta_lag_periods: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]
  z_anchored_ratio_lag_periods: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
  z_adjacent_ratio_lag_periods: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] # [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]
  z_regression_slopes_window_sizes: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] # [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
  stocks_file: "tb_stocks.txt"
  products_file: "tb_productos.txt"
  sell_in_file: "sell-in.txt"
  products_for_kaggle_file: "product_id_to_predict_201912.txt"
  target: "target"
  target_type: "std" # "std" "diff_std"
  period: "periodo"
  cat_features:
    - cat1
    - cat2
    - cat3
    - brand
    - descripcion
  dataset_name: "dataset.parquet"
  products_12m_agg_dataset_name: "products_12m_agg.parquet"
  clipping_threshold: 1e7  # Extreme value clipping threshold
  add_12m_sarima_features: true  # Add SARIMA features trained on 12 months of data with a 12-month seasonal cycle

strategy:
  test_month: 201910 # int
  kaggle_month: 201912 # int

cv:
  n_folds: 5

optimizer:
  study_name: "demand_forecasting_study"
  direction: "minimize"
  n_trials: 16

  param_ranges:
    learning_rate: [0.03, 0.07]  # log scale
    num_leaves: [500, 700]  # int
    max_depth: [8, 25]  # int
    min_data_in_leaf: [6500, 11000]  # int
    min_sum_hessian_in_leaf: [0.01, 0.5]  # log scale
    feature_fraction: [0.7, 1.0]  # float
    lambda_l1: [0.001, 0.5]  # log scale
    lambda_l2: [0.001, 0.5]  # log scale
    linear_lambda: [0.1, 0.5]  # log scale
    max_bin: [350, 600]  # int

  base_model_params:
    objective: 'regression'
    num_iterations: 9999 # Big enough to let early stopping work
    linear_tree: True
    seed: 42
    feature_fraction_seed: 42
    verbosity: -1
    metric: 'custom'

final_train:
  num_seeds: 16